{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport random\nimport time\nimport requests\nimport re\nimport os\nfrom io import StringIO\n\n# Import column dictionary\nfrom column_dictionary import (\n    TABLE_CONFIGS,\n    get_column_mapping,\n    get_headers,\n    needs_header_fix,\n)\n\n# Output directory\nOUTPUT_DIR = \"temp_csv\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# NFL team abbreviations (Pro-Football-Reference format)\nnfl_teams = [\n    \"crd\", \"atl\", \"rav\", \"buf\", \"car\", \"chi\", \"cin\", \"cle\",\n    \"dal\", \"den\", \"det\", \"gnb\", \"htx\", \"clt\", \"jax\", \"kan\",\n    \"rai\", \"sdg\", \"ram\", \"mia\", \"min\", \"nwe\", \"nor\", \"nyg\",\n    \"nyj\", \"phi\", \"pit\", \"sfo\", \"sea\", \"tam\", \"oti\", \"was\"\n]\n\n# URLs\nteam_rankings_url = \"https://www.pro-football-reference.com/years/2025/\"\ndefensive_rankings_url = \"https://www.pro-football-reference.com/years/2025/opp.htm\"\n\ndef get_team_profile_url(team_abbr):\n    return f\"https://www.pro-football-reference.com/teams/{team_abbr}/2025.htm\"\n\n\ndef clean_dataframe(df, table_type):\n    \"\"\"\n    Clean a dataframe by fixing headers and renaming columns.\n    \n    Args:\n        df: The dataframe to clean\n        table_type: The type of table (e.g., 'passing_offense', 'schedule')\n    \n    Returns:\n        Cleaned dataframe\n    \"\"\"\n    # Get column mapping\n    column_mapping = get_column_mapping(table_type)\n    \n    # Rename columns that exist in the mapping\n    rename_dict = {old: new for old, new in column_mapping.items() if old in df.columns}\n    if rename_dict:\n        df = df.rename(columns=rename_dict)\n    \n    return df\n\n\ndef extract_tables(url, tables, output_dir=OUTPUT_DIR, prefix=\"\"):\n    \"\"\"\n    Extract tables from any Pro-Football-Reference URL.\n    Automatically handles tables hidden in HTML comments.\n    Fixes headers and renames columns using column_dictionary.\n    Skips files that already exist.\n    \n    Args:\n        url: The page URL to scrape\n        tables: Dict mapping table_id -> (filename, table_type)\n        output_dir: Directory to save CSV files\n        prefix: Optional prefix for filenames (e.g., team abbr)\n    \n    Returns:\n        Dict of successfully extracted DataFrames\n    \"\"\"\n    results = {}\n    tables_to_fetch = {}\n    \n    # Check which files already exist\n    for table_id, (filename, table_type) in tables.items():\n        full_filename = f\"{prefix}{filename}\" if prefix else filename\n        filepath = f\"{output_dir}/{full_filename}.csv\"\n        if os.path.exists(filepath):\n            print(f\"⏭ Skipping {full_filename}.csv (already exists)\")\n        else:\n            tables_to_fetch[table_id] = (full_filename, table_type)\n    \n    # If all files exist, skip the HTTP request\n    if not tables_to_fetch:\n        return results\n    \n    # Fetch the page\n    response = requests.get(url)\n    html = response.text\n    \n    # Extract HTML comments (where PFR hides some tables)\n    comments = re.findall(r'<!--(.+?)-->', html, re.DOTALL)\n    \n    for table_id, (filename, table_type) in tables_to_fetch.items():\n        df = None\n        \n        # Determine header row based on table type\n        header_row = 0 if needs_header_fix(table_type) else 1\n        \n        # Try 1: Direct extraction from visible HTML\n        try:\n            df = pd.read_html(StringIO(html), header=header_row, attrs={'id': table_id})[0]\n        except (ValueError, IndexError):\n            pass\n        \n        # Try 2: Extract from HTML comments\n        if df is None:\n            for comment in comments:\n                if f'id=\"{table_id}\"' in comment:\n                    try:\n                        df = pd.read_html(StringIO(comment), header=header_row, attrs={'id': table_id})[0]\n                        break\n                    except (ValueError, IndexError):\n                        continue\n        \n        # Process and save if found\n        if df is not None:\n            # Clean the dataframe (rename columns)\n            df = clean_dataframe(df, table_type)\n            \n            # Save to CSV\n            filepath = f\"{output_dir}/{filename}.csv\"\n            df.to_csv(filepath, index=False)\n            results[filename] = df\n            print(f\"✓ Saved {filename}.csv ({len(df)} rows, {len(df.columns)} cols)\")\n        else:\n            print(f\"✗ Table '{table_id}' not found\")\n    \n    return results\n\n\n# =============================================================================\n# 1. TEAM RANKINGS (Offensive)\n# =============================================================================\n# Format: table_id -> (filename, table_type)\nranking_tables = {\n    \"team_stats\": (\"team_offense\", \"team_offense\"),\n    \"passing\": (\"passing_offense\", \"passing_offense\"),\n    \"rushing\": (\"rushing_offense\", \"rushing_offense\"),\n    \"team_scoring\": (\"scoring_offense\", \"scoring_offense\"),\n    \"AFC\": (\"afc_standings\", \"afc_standings\"),\n    \"NFC\": (\"nfc_standings\", \"nfc_standings\"),\n}\n\nprint(\"=\" * 50)\nprint(\"FETCHING TEAM RANKINGS (OFFENSE)\")\nprint(\"=\" * 50)\nresults = extract_tables(team_rankings_url, ranking_tables)\nprint(f\"Extracted {len(results)} tables.\\n\")\n\n# =============================================================================\n# 2. DEFENSIVE RANKINGS\n# =============================================================================\ndefensive_tables = {\n    \"team_stats\": (\"team_defense\", \"team_defense\"),\n    \"advanced_defense\": (\"advanced_defense\", \"advanced_defense\"),\n    \"passing\": (\"passing_defense\", \"passing_defense\"),\n    \"rushing\": (\"rushing_defense\", \"rushing_defense\"),\n}\n\nprint(\"=\" * 50)\nprint(\"FETCHING DEFENSIVE RANKINGS\")\nprint(\"=\" * 50)\nresults = extract_tables(defensive_rankings_url, defensive_tables)\nprint(f\"Extracted {len(results)} tables.\\n\")\n\n# =============================================================================\n# 3. TEAM PROFILES (32 teams)\n# =============================================================================\n# Format: table_id -> (filename, table_type)\nprofile_tables = {\n    \"team_stats\": (\"team_stats\", \"team_stats\"),\n    \"games\": (\"schedule\", \"schedule\"),\n    \"passing\": (\"passing\", \"passing\"),\n    \"rushing_and_receiving\": (\"rushing_receiving\", \"rushing_receiving\"),\n    \"defense\": (\"defense\", \"defense\"),\n    \"scoring\": (\"scoring\", \"scoring\"),\n    \"team_td_log\": (\"touchdown_log\", \"touchdown_log\"),\n}\n\ndef get_profile_tables_for_team(team_abbr):\n    tables = profile_tables.copy()\n    tables[f\"{team_abbr}_injury_report\"] = (\"injury_report\", \"injury_report\")\n    return tables\n\nprint(\"=\" * 50)\nprint(\"FETCHING TEAM PROFILES (32 teams)\")\nprint(\"=\" * 50)\n\nfor i, team in enumerate(nfl_teams):\n    print(f\"\\n[{i+1}/32] {team.upper()}\")\n    print(\"-\" * 30)\n    \n    url = get_team_profile_url(team)\n    tables = get_profile_tables_for_team(team)\n    \n    results = extract_tables(url, tables, prefix=f\"{team}_\")\n    \n    # Rate limiting - 3 seconds between teams\n    if i < len(nfl_teams) - 1:\n        time.sleep(3)\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"DONE!\")\nprint(\"=\" * 50)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}